def prompt_evaluation(llm, chains, data_fetcher, year, month):
    # Define financial report components prompting approaches
    components = {
        "executive_summary": ["zero_shot", "few_shot", "chain_of_thought"]
    }
    
    # Initialize result structure
    results = {}
    for component, approaches in components.items():
        results[component] = {}
        for approach in approaches:
            results[component][approach] = {
                "success_rate": 0,
                "reponse_time": 0,
                "cost": 0,
                "personalize_score": 0
                }
    
    # Store outputs for similarity analysis
    outputs_by_component = {}
    for component in components:
        outputs_by_component[component] = {}
        for approach in components[component]:
            outputs_by_component[component][approach] = []
    
    # Define a nested dictionary, storing outputs generated by LLM
    # Organized by user segment, report component, prompting approach
    outputs_by_segment = {}
    
    user_ids = data_fetcher.user_ids
    
    # Loop through userID, fetching Transaction Summary for target year and month
    for user_id in user_ids:
        transaction_summary = data_fetcher.monthly_profile(year=year, month=month, user_id=user_id)
        # Define a dictionary storing transaction summary and user segment
        user_data = {
            "summary": transaction_summary,
            "segment": data_fetcher.data[data_fetcher.data['user_id'] == user_id].iloc[0]['segment_tag']
        }
        # User segment tag
        user_segment = user_data["segment"]
        # Create empty list to store generated output by user segment, report component, prompting approach
        if user_segment not in outputs_by_segment:
            outputs_by_segment[user_segment] = {}
            # Loop through Financial Report components (executive summary, cash flow analysis, transaction behaviour, saving, recommendations)
            for component in components:
                outputs_by_segment[user_segment][component] = {}
                # Loop through each prompting approach (if applicable)
                for approach in components[component]:
                    outputs_by_segment[user_segment][component][approach] = []
    
    # Prepare inputs
    inputs = {"transaction_summary": transaction_summary}
    
    # Calculate input tokens for cost estimation
    input_text = ""
    
    
    
    
    
    
    # Cost by Claude 3.7 Sonnet model
    cost_per_input_token = 0.000015
    cost_per_output_token = 0.000075
    chars_per_token = 4 # Assuming ~4 characters per token
            
    # Calculate averages for success rate, response time and cost
    for component, approaches in components.items():
        for approach in approaches:
            total_tests = len(user_ids)
            if total_tests > 0:
                results[component][approach]["success_rate"] /= total_tests
                results[component][approach]["reponse_time"] /= total_tests
                results[component][approach]["average_cost_per_test"] = results[component][approach]["cost"] / total_tests
            else:
                results[component][approach]["average_cost_per_test"] = 0 # Avoid division by zero
        